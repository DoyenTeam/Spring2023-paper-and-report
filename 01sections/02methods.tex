\section{Methods}

PubMed supports the mass download of their article data in over one thousand 10-30 MB compressed archives stored on an FTP server. To populate and update our index, we created a reusable and configurable pipeline that ingests the archives, decompresses them, and enters all the individual articles into our data store. This is also a key point of data sanitation and enhancement, where we can remove faulty entries, improve the quality of existing data, and potentially augment the data with additional sorting or external sources. 

Our application needed to be able to respond to a user's query as fast as possible, and the first part of that involves finding articles relevant to the expertise the user wants to find. We use Elasticsearch to build an index over the abstracts, titles, and metadata provided by PubMed. When a user performs a search in Elasticsearch, the system returns a list of documents that match their search terms, ranked by their relevance score. The relevance score measures how closely each document matches the user's search terms, with higher scores indicating more relevant documents. 

To calculate the relevance score, Elasticsearch takes into account several factors. One of the most important is the frequency of the search terms in the document. If a search term appears more often in a document, Elasticsearch considers that document to be more relevant to the user's search query. Another critical factor is the rarity of the search terms across the entire dataset. If a search term is rare and appears in only a few documents, Elasticsearch considers those documents more relevant to the user's search query. Elasticsearch can also consider the boosts we assign to specific fields in the search query. 

Overall, the relevance score is a measure of how closely each document matches the user's search terms based on a variety of factors, including the frequency of the search terms in the document, the rarity of the search terms across the entire dataset, the length of the fields in the documents, and any boosts the user has assigned to specific fields. The higher the relevance score, the more closely the document matches the user's search terms, and the more likely it is to be a relevant result. All scores are relative and hold meaning only within the context of a particular search. 

Querying Elasticsearch directly is a skill in itself, however, and the data is still organized by articles, so we implemented a REST API to turn simple natural English keyword searches into Elasticsearch queries. The API also converts the list of articles into a list of authors, which are ranked by the relevancy of their work to the given search terms, the year their work was published, citations, and other sort options and filters. We compute a relevancy score $S_a$ for each author $a$, using the Elasticsearch scores $R_{a, p}$ for each of the papers $p$ affiliated with that author that appears in the search:
$$
S_a = \sum_{p} R_{a, p}
$$
This formulation favors authors with many highly relevant papers. More papers mean more terms, and each term weights the importance of each paper. Note that the authors listed on the paper are all treated equally. We need a way to distinguish levels of contribution, which we discuss further when we discuss Limitations. 

To make the application easy to use, we created a responsive browser application that can be accessed anywhere with internet access. The app provides users with options for entering their search terms describing the expertise they want to find, either as MeSH terms or as general keywords. A key advantage of Elasticsearch is that we can search for any word appearing in abstracts and titles and are not restricted to MeSH terms alone. 