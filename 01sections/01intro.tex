\section{Introduction}

Ascertaining the expertise of individuals within a given subject matter domain poses a significant challenge in both academic and professional contexts. Evidence-based research has become the cornerstone of almost every sector of economic growth in the industry while remaining a principal source of personal and cultural growth in the academic arts and sciences. Large organizations like governments, corporations, and academic institutions have resources to spare and established networks to draw on. They can draw experts to themselves with grants and access to other experts. However, doctors, students, professionals, and private individuals with no particular expertise may need experts far outside their networks. Health professionals might need an expert for a second opinion or a collaborator in a novel case study. A professional working in R\&D may need an expert opinion to gauge the value and viability of a new product line. Regardless of the use case, it is challenging to find individuals with subject matter experience and, even more so, determine which of them are subject matter experts. 

There are two stages to finding an expert. First, one needs to find individuals who have worked within the subject matter, and second, one must find the actual experts. Correspondingly, one might face two kinds of problems: either there are few or no experts for a subject matter because it is so narrow, or it is broad enough that there are too many experts to choose from easily. These problems are evidenced when attempting to extract experts from academic literature sources. Furthermore, these issues are present within specific domains of literature research, such as PubMed, a tool for searching biomedical and life sciences literature. 

PubMed natively offers literature search features based on specific user needs to find individuals with work published in a given subject matter. The Clinical Queries feature provides a search function with predefined filters for users to select and refine the results of relevant publications. The Single Citation Matcher tool allows citation searches based on specific user inputs and returns relevant citations of specific publications found on PubMed \cite{ref-pubmed-native}. 

PubMed articles are indexed using Medical Subject Heading (MeSH) terms. MeSH terms provide the ability to catalog biomedical publications and serve as powerful keywords used to search the PubMed database. Citation Matcher allows users to access and search within the database of MeSH terms. In addition, PubMed's Advanced Search feature allows users to build boolean queries using common terms and phrases translated into corresponding MeSH terms. However, PubMed's translation algorithm, used to convert query text into a relevant MeSH term, is imperfect, so retrieving relevant results may prove challenging if users are not well-versed in synonymically searching the 60,000 MeSH terms available \cite{ref-pubmed-updated}. Moreover, PubMed focuses on searching and retrieving literature publications rather than the experts behind the literature. While the search features are robust, PubMed's focus on literature retrieval necessitates a deeper investigation into the identification and ranking of experts. 

When faced with many authors associated with many results, ranking them as a way to narrow the ``foremost experts'' in a field can be helpful. There are numerous metrics commonly used to evaluate academic expertise. Most are simple counts, such as the number of publications or citation counts. However, awards and grant income are also widely used, and more complex metrics (often mathematical combinations of counts) are widely used. However, the limitations of numerical metrics to consistently and accurately measure expertise and ``value'' are widely acknowledged in the literature. Any single number does not tell a complete story and can often be misleading without further context \cite{ref-metrics-stack-overflow}. When metrics are combined to form comprehensive scores, they often embed subjective concepts of value that are particular to the academic landscape of the moment \cite{ref-metrics-neoliberalism}. Many metrics of value rely on the recent evaluation of the work, either directly or indirectly, so novel work that challenges current thinking may be overlooked until it becomes more widely circulated. Similarly, expert and careful work to confirm previous experiments may demonstrate a quality researcher but be overlooked because it is not flashy or new \cite{ref-metrics-games-academics-play}.

The Doyen team has created a tool that improves the issues of finding any expert in a narrow subject and finding an expert when a broad subject offers many experts. To that end, we provided a tool allowing users to search for an ``expert'' by providing adjustable post-search filters that the user deems most relevant. A government organization might value different criteria than a health professional. For example, a government organization might require that an individual be a national, while a health professional may require certain degrees and certifications. Hence, we made no intention to define what an expert should be in all contexts. We also limited searches to biomedical fields, allowing us to use the PubMed database, which contains more than 30 million publications that have been professionally indexed. Currently, we are utilizing roughly 1/3 of the most recent publications. 


\subsection{Existing Tools and Methods for Mining Publications to Find Experience}

First, we will consider what tools exist for finding experienced individuals and, in some cases, subject matter experts. Over the years, PubMed has expanded its search and filter options, yet the number of results can be overwhelming to whittle down into valuable results. To this end, developers have used PubMed's freely available API tools, such as E-Utilities and FTP, to build alternatives to searching PubMed natively. However, these tools, like PubMed, focus on searching and filtering the publications themselves rather than publication authors. One study surveyed widely used third-party PubMed tools \cite{ref-pubmed-tools}. Some tools aim to rank results or provide a better user experience, such as MiSearch and SLIM, that rank PubMed results based on user-defined weights and slider presets \cite{ref-pubmed-misearch}. Other tools seek to group results into topics or helpfully visualize outputs for users. The Anne O'Tate tool is an excellent example of both, offering users a result dashboard of relevant articles, an author list, and relevant MeSH terms associated with each. Every dashboard item displays as a link, and when clicked, a new PubMed search is performed in the background using the clicked data, giving a drill-down option into results without the user having to build multiple new searches from scratch \cite{ref-pubmed-anne-otate}. 

Several PubMed search alternatives consider options to mark and highlight search results. PubTator and PubTerm tackle this issue differently; the former offers predefined color-highlighted results based on topics, while the latter lets users make savable highlights and notes to their results as they see fit \cite{ref-pubmed-pubterm, ref-pubmed-pubtator}. Tools like LAITOR offer highly targeted queries but require software-savvy users to run MySQL queries and PHP scripts, while other tools, such as PESCADOR, abstract the need to code and make them more user-friendly \cite{ref-pubmed-pescador}.

Several alternatives combine PubMed data with outside resources to offer richer search results. For example, one tool added the ORCID database and the Elsevier API to assign author IDs to search results \cite{ref-pubmed-pmsc}. It then ranked the results by relevance and replaced the article titles with the author names, so users could easily see ranked authors based on search topics. Another tool combined the PubMed API, MeSH database, and the Orphadata directory to create author profiles dealing with rare diseases \cite{ref-pubmed-bibiometric}. Each author's profile includes their top publications on rare diseases and the top MeSH terms associated with their work. Admittedly, this tool took a naive approach to name disambiguation, combining multiple authors sharing a last name and initials. 

Other tools further combined PubMed data with outside resources by adding machine learning to filter results. One promising tool, PKG, built a knowledge graph using PubMed abstracts, NIH's ExPORTER funding information, education data from ORCID, and affiliation data from MapAffil \cite{ref-pubmed-knowledge-graph}. MapAffil is a bibliographic tool for mapping author affiliation strings to cities and their geocodes \cite{ref-pubmed-mapaffil}. The team then used a deep learning biomedical text mining model (BioBERT) to train collected data, build connections, and populate the knowledge graph. 

Like systems recommending products or entertainment, content-based filtering recommendation systems have also been used to suggest experts. Content-based filtering uses data feature vectors to make recommendations based on the user's explicit actions, such as a retail purchase or a movie watched. The model is crafted with similarity metrics and manually selected feature sets deemed suitable for the problem, requiring domain knowledge \cite{ref-google-content-filtering}. With respect to an expert finding using PubMed, the database is mined to produce features associated with authors of publications. As the user searches and finds experts, the system learns which characteristics and metrics are important to the user in evaluating expertise. 

Collaborative filtering is another machine-learning technique used to find patterns in the data. Concerning expert-finding of PubMed, this would evaluate the patterns between the user's search preferences or filters that have been previously with the experts' data and their publications to recommend a desirable match. The benefit of this type of system is that there is no need to specify the features of publications which suggest expertise. The patterns of established experts could suggest emerging experts in a field. However, the drawback is the cold start problem of initializing through explicit data before recommendations will start to become meaningful \cite{ref-google-collaborative-filtering}.

An approach to expert finding published in Applied Intelligence is a hybrid between content-based and collaborative filtering recommendation systems aiming to improve accuracy by deriving keywords from experts' Wikipedia references and using them to identify communities of experts to recommend as a set to the user. This approach uses a semantic-based method to derive expert social network relations. A k-means clustering algorithm with a proprietary homogeneity measurement was used to determine the weighted values between experts in a detected community \cite{ref-semantic-social-network}. In the PubMed Expert \& Journal Tool context, this approach would be similar to connecting experts via MeSH - Medical Subject Headings - terms and constructing a social network via similarities rather than a direct connection of terms. Although this type of approach has upsides, it, too, requires a value judgment during development regarding how to weigh similarities. 

A novel approach to using social networks to identify experts is a propagation-based approach proposed by researchers at Tsinghua University. Like PageRank, this approach makes recommendations based on their links to other experts after propagation. Unlike early PageRank, they tackled the problem where the graph nodes with the highest in-degree tend to end up in results most frequently by beginning with a local score for an expert based on the features of their work before using propagation to determine their global score. Additionally, they weight the edges of the social network graph using a proprietary algorithm to determine the propagation effectiveness \cite{ref-expert-social-network}. A significant drawback to this approach regarding PubMed Expert \& Journal Tool is that it requires its development team to assert a value judgment on the local scores and propagation coefficients. 

Despite the wide use of third-party PubMed tools, not all are convinced they enhance PubMed searches. A 2016 study comparing native PubMed to third-party search tools found that of the 76 third-party tools available, only 16 were free to access \cite{ref-pubmed-third-party}. Furthermore, those 16 offered minimal user control of parameters, filtering, and exporting compared to native PubMed. Today, many tools explored in the comparison study are either out of date or inactive. 

\subsection{Our Approach}

Ultimately, we have provided the biomedical community with an up-to-date tool that provides control of parameters and filters to find field experts. The value of our tool stands in allowing a way to search for authors in a field, as opposed to publications â€“ which PubMed currently allows through various methods. We also leveraged the power of Elasticsearch to supply an easy-to-use yet powerful engine for finding the expertise the user requires. Elasticsearch is a distributed data search and analytics engine built for speed and scalability \cite{ref-elasticsearch}. We used Elasticsearch to index the data provided by PubMed, which we can then semantically search with keywords and MeSH terms to find publications relevant to even the most narrow specifications if they exist. Using our intermediary API, we can filter and sort these results into lists of expert authors based on user-specified criteria. We have exposed this functionality through a new user-friendly browser-based application. In addition, we aim to engage the community to receive feedback and allow others to improve and grow the project, so all our code is open-source. 