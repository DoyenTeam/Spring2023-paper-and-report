\section{Discussion}

We created a solid foundation for this application with valuable core features. We used the rapid search and scoring features of Elasticsearch to provide complete flexibility in the words a user can use to search for expertise. We were able to avoid the limitations of being restricted to the MeSH ontology while still being able to make use of it when applied to a user's search terms.

However, this is just the beginning of the app's potential in many ways. Many other sources contain explicit or implicit information on the expertise of academics and professionals. In addition, our approach has some critical limitations that do not make full use of the data we already have access to. Lastly, considering the ethics of ranking experts for the benefit of industry talent hunters and funding agencies leads to novel approaches to the standard paradigm of a recommendation or search system. 

\subsection{Challenges Ranking Authors}

The current application does not have a way to disambiguate authors, which means that common names can dominate results. This would best be addressed using existing solutions for author name disambiguation \cite{ref-approach-author-name-disambiguation, ref-openalex}. We could also do more to map general terms into MeSH identifiers \cite{ref-approach-gilda}. However, we found that many of the queries our users wanted to make could not be converted into MeSH IDs. 

We have also not made any distinction between the authors on a paper. It is no secret that some authors will have been deeply involved in the typical paper, while others may have been only tangentially part of the process. In the strictest sense, some of this information is irretrievable without the full text of an article, an ``author contributions'' section, or similar. However, some emphasis can be inferred from the order and number of authors. The inclusion of information about the role of the author in their institution could also suggest prominence, as, for example, the heads of labs often play only an advisory role.

It is also not hard to find combinations of criteria that yield no results because there are no authors with that exact overlapping experience. We suggest the novel approach of suggesting an optimized group of individuals to meet over-constrained criteria, thus allowing us to supply a response no matter how narrow and niche the requirements are. Doyen could use the institution data present in PubMed to let users search based on the physical locations of authors and could use external sources of metric data to provide other filters for author credibility and prominence. 

\subsection{Ethical Considerations in Ranking}

As the application grows its user base, it will be essential to consider challenging ethical concerns. However, a careful examination of these ethical considerations will also suggest novel solutions that add unique value to the users of this application. 

Given a large selection of individuals with experience in a subject matter, it is necessary to select a subset of that list to return to our users. A simple method to do that is to rank experts. Ranks can be as simple as counts of publications, citations, grants, or awards. A commonly used and slightly more abstract number, the h-factor or h-index, combines citation counts and the number of publications \cite{ref-metrics-methods-web}. Variants such as the m-index, which accounts for the length of the scientist's career, have also been proposed, though they appear to have minimal hold based on their absence from most of the literature \cite{ref-metrics-using-metrics}. Other numbers can be used to rank academics on their productivity and expertise, many of them domain-specific, and systems have been proposed to assemble these numerical measures into singular ranks \cite{ref-metrics-eval-biostat}. However, few seem to have caught on systemically (however grandiose the authors' claims). 

Our research found that the attitude towards ranking and scoring in academia has soured over the last few decades. Very little has recently been published proposing new metrics, with the most recent publications appearing in the mid-2010s. Meanwhile, metrics are frequently and increasingly used to filter out candidates for grants and jobs. This is undoubtedly a major cause for the shift toward critiquing the metric process in academic publications. Key among these criticisms is the way that metrics have been gamified \cite{ref-metrics-manipulation, ref-metrics-games-academics-play}, how they create perverse incentives that reduce the quality of academic work \cite{ref-metrics-neoliberalism}, and our ability to separate experts from frauds \cite{ref-metrics-perverse-incentives}. Simply making metrics readily available and browsable can have a negative effect on relationships between academics, and the context and purpose of such metric displays must be carefully considered when creating such applications \cite{ref-metrics-resviz}.

Attempts have been made to create more comprehensive, complete, or sophisticated metrics. Some have used document metrics, including computational analysis techniques, and considering details such as author status on a paper. The more sophisticated of these systems have become paid products, such as the Web of Science, now owned by Clarivate (previously the IP of Thomson Reuters) \cite{ref-metrics-using-metrics}. Other attempts have been made to establish if a person is ``expert enough'' acknowledging the lack of any single ``gold standard,'' many of them using the so-called Cochran-Weiss-Shanteau (CWS) method \cite{ref-approach-to-identifying-smes}. Some of these have been aimed more at in-house employee rankings and evaluations and thus assume the ability to acquire specific input from the individuals in question. They attempted to improve on simple metrics by utilizing employee responses, then using statistical analysis to extract scores from those responses \cite{ref-metrics-cws-guide}, or using collaborator responses to create measures of ``success,'' including social disposition \cite{ref-metrics-psycho-scale}.

It is the responsibility of funding agencies and, by extension, any service that aims to serve them, to make decisions that improve the culture of practicing science \cite{ref-metrics-games-academics-play}. It is fundamentally difficult to judge the work of experts by the very nature of being experts because, by definition, an expert can do something few others can. Thus few others are qualified to judge them. Moreover, long, nuanced, qualitative descriptions of expertise are difficult to compare. This is why metrics are valuable and sought after in the first place; they are practically necessary. Metrics and rankings are fraught with limitations, creating perverse incentives that lead to the distortion of the metrics and diluting their actual value. An application that claims to identify expert individuals must balance the tension between easy evaluation, resistance to malign manipulation, and fostering adversarial behavior amongst the subjects of evaluation. 

\subsection{Future Work}

In addition to addressing the key limitations, some novel strategies are worth considering, and the deployment and infrastructure can be improved and streamlined. For a publicly accessible and likely publicly funded project such as this, it will be essential to limit the cost of keeping the service running. To that end, the cloud utilization could be consolidated into a single provider. Future efforts would also benefit from using cheaper serverless deployments of the API and using the scalability of compute resources to use only as much computing power as is needed. Specifically, the pipeline that streams PubMed data into Elasticsearch requires significant computing power but only runs a few hours each week. 

Future work could also use the information about an author's research institution provided by PubMed to filter by the author's location. The primary challenge here is the poor quality of the data provided by PubMed, with some institution information blended with author emails and very infrequent institution IDs. Here again, integrating with an external source such as OpenAlex could help to resolve these ambiguities, allowing users to select experts based on proximity. 

Lastly, inspired by ethical considerations, it would be worth considering non-ranking or quasi-random strategies for suggesting experts. Rather than simply returning the best matches or the top scores, one could randomly select a result or select a result systematically to ensure results do not repeat and which at least mostly ignores the rank or score of users. So long as the result meets the user's criteria, it may not be necessary to offer the best result but rather a good enough result. This could also unlock considerable performance improvements by reducing sort computations, which are intrinsically global, comparing each result against every other. Last but not least, this could help ensure that, should this app become widely used, the load of requests for expert opinions would be more evenly distributed across the network of experts. 